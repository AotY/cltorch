#include "THClApply.h"

// Implementation of copyIgnoringOverlaps, defined after pointwiseApply2.
void THClTensor_copyIgnoringOverlaps(THClState* state,
                                       THClTensor* dst,
                                       THClTensor* src) {
  THClTensor_pointwiseApply2(state, dst, src, CopyOp<float>(),
                               ReadOnly, // ignore overwrites
                               ReadOnly);
}

std::string getApplyD_template() {
    // [[[cog
    // import stringify
    // stringify.write_kernel( "kernel", "THClApplyD.cl" )
    // ]]]
    // generated using cog, from THClApplyD.cl:
    const char * kernelSource =  
    "// OpenCL kernels....\n" 
    "\n" 
    "// expected templated values:\n" 
    "// dims (vector of unique dimension values)\n" 
    "// operation\n" 
    "// dim1\n" 
    "// dim2\n" 
    "// dim3\n" 
    "// ... dimD\n" 
    "// num_input_tensors\n" 
    "// include_scalar_input\n" 
    "//\n" 
    "// maybe should add:\n" 
    "// IndexType (hardcoded to int for now)\n" 
    "// MAX_CUTORCH_DIMS (hardcoded to 25 for now)\n" 
    "\n" 
    "// (Ported from cutorch's THCApply.cuh)\n" 
    "\n" 
    "// Maximum number of dimensions allowed for cutorch\n" 
    "// #define MAX_CUTORCH_DIMS 25\n" 
    "\n" 
    "// Enum that indicates whether tensor arguments are read/write or\n" 
    "// read-only\n" 
    "//enum TensorArgType { ReadWrite, ReadOnly };\n" 
    "\n" 
    "// kernel argument that defines tensor layout\n" 
    "typedef struct TensorInfoCl {\n" 
    "  // Extracts size/stride information for the kernel.\n" 
    "  // Successive dimensions can be collapsed if the size/strides match\n" 
    "  // up and thus there are no holes between the dimensions. This is used\n" 
    "  // to reduce the complexity of the problem.\n" 
    "  // The optional `reduceDim` indicates a reduction dimension for the\n" 
    "  // given tensor, so that the output size for this dimension will be 1.\n" 
    "\n" 
    "  int sizes[{{MAX_CLNN_DIMS}}];\n" 
    "  int strides[{{MAX_CLNN_DIMS}}];\n" 
    "  int offset;\n" 
    "  int dims;\n" 
    "} TensorInfoCl;\n" 
    "// Contiguous tensors of more than one dimension are collapsed down\n" 
    "// to one tensor\n" 
    "bool TensorInfo_isContiguous( TensorInfoCl tensorInfo ) {\n" 
    "    return (tensorInfo.dims == 1 && tensorInfo.strides[0] == 1);\n" 
    "}\n" 
    "\n" 
    "{%\n" 
    " total_opsize = num_tensor_inputs\n" 
    " if include_scalar_input then\n" 
    "      total_opsize = total_opsize + 1\n" 
    "   end\n" 
    " %}\n" 
    "\n" 
    "void op( global float *out, global float *in1 ) {\n" 
    "    {{operation}};\n" 
    "}\n" 
    "\n" 
    "// Translate a linear index for the apply to a float* offset;\n" 
    "// specialized on `Dims` to reduce nvcc compilation time\n" 
    "{% for _,dim in ipairs(dims) do %}\n" 
    "int IndexToOffset_{{1000 + dim}}_get( int linearId, TensorInfoCl info) {\n" 
    "  int offset = 0;\n" 
    "\n" 
    "  // Use static dims\n" 
    "  for (int i = {{dim}} - 1; i >= 0; --i) {\n" 
    "    int curDimIndex = linearId % info.sizes[i];\n" 
    "    int curDimOffset = curDimIndex * info.strides[i];\n" 
    "    offset += curDimOffset;\n" 
    "\n" 
    "    if (i > 0) {\n" 
    "      linearId /= info.sizes[i];\n" 
    "    }\n" 
    "  }\n" 
    "\n" 
    "  return offset;\n" 
    "}\n" 
    "{% end %}\n" 
    "\n" 
    "int IndexToOffset_998_get(int linearId, const TensorInfoCl info) {\n" 
    "    return linearId;\n" 
    "}\n" 
    "\n" 
    "int IndexToOffset_999_get(int linearId, const TensorInfoCl info) {\n" 
    "  int offset = 0;\n" 
    "\n" 
    "  // Use dynamic dims\n" 
    "  for (int i = info.dims - 1; i >= 0; --i) {\n" 
    "    int curDimIndex = linearId % info.sizes[i];\n" 
    "    int curDimOffset = curDimIndex * info.strides[i];\n" 
    "    offset += curDimOffset;\n" 
    "\n" 
    "    linearId /= info.sizes[i];\n" 
    "  }\n" 
    "\n" 
    "  return offset;\n" 
    "}\n" 
    "\n" 
    "kernel void\n" 
    "THClTensor_pointwiseApplyD(\n" 
    "   {% for input_idx=1,num_tensor_inputs do %}\n" 
    "    global TensorInfoCl *info_{{input_idx}},\n" 
    "    global float*data_{{input_idx}},\n" 
    "   {% end %}\n" 
    "   {% if include_scalar_input then %}\n" 
    "   float value,\n" 
    "   {% end %}\n" 
    "   int totalElements) {\n" 
    "  for (int linearIndex = get_global_id(0);\n" 
    "       linearIndex < totalElements;\n" 
    "       linearIndex += get_global_size(0) /* ? */ ) {\n" 
    "    {% for input_idx=1,num_tensor_inputs do %}\n" 
    "    // Convert `linearIndex` into an offset of `a`\n" 
    "    const int offset{{input_idx}} =\n" 
    "      IndexToOffset_{{1000+loadstring('return dim' .. input_idx)()}}_get(linearIndex, info_{{input_idx}}[0]);\n" 
    "    {% end %}\n" 
    "\n" 
    "    op(\n" 
    "      {% for input_idx=1,num_tensor_inputs do %}\n" 
    "         {% if input_idx > 1 then %} , {% end %}\n" 
    "         &(data_{{input_idx}}[offset{{input_idx}} + info_{{input_idx}}->offset])\n" 
    "      {% end %}\n" 
    "      {% if include_scalar_input then %}\n" 
    "      , &value\n" 
    "      {% end %}\n" 
    "    );\n" 
    "  }\n" 
    "}\n" 
    "\n" 
    "";
    // [[[end]]]
  return kernelSource;
}

std::string getApply1_template() {
    // [[[cog
    // import stringify
    // stringify.write_kernel( "kernel", "THClApply1.cl" )
    // ]]]
    // generated using cog, from THClApply1.cl:
    const char * kernelSource =  
    "// OpenCL kernels....\n" 
    "\n" 
    "// expected templated values:\n" 
    "// dims (vector of unique dimension values)\n" 
    "// operation\n" 
    "// adim\n" 
    "// bdim\n" 
    "// cdim\n" 
    "//\n" 
    "// maybe should add:\n" 
    "// IndexType (hardcoded to int for now)\n" 
    "// MAX_CUTORCH_DIMS (hardcoded to 25 for now)\n" 
    "\n" 
    "// (Ported from cutorch's THCApply.cuh)\n" 
    "\n" 
    "// Maximum number of dimensions allowed for cutorch\n" 
    "// #define MAX_CUTORCH_DIMS 25\n" 
    "\n" 
    "// Enum that indicates whether tensor arguments are read/write or\n" 
    "// read-only\n" 
    "//enum TensorArgType { ReadWrite, ReadOnly };\n" 
    "\n" 
    "// kernel argument that defines tensor layout\n" 
    "typedef struct TensorInfoCl {\n" 
    "  // Extracts size/stride information for the kernel.\n" 
    "  // Successive dimensions can be collapsed if the size/strides match\n" 
    "  // up and thus there are no holes between the dimensions. This is used\n" 
    "  // to reduce the complexity of the problem.\n" 
    "  // The optional `reduceDim` indicates a reduction dimension for the\n" 
    "  // given tensor, so that the output size for this dimension will be 1.\n" 
    "\n" 
    "  int sizes[{{MAX_CLNN_DIMS}}];\n" 
    "  int strides[{{MAX_CLNN_DIMS}}];\n" 
    "  int offset;\n" 
    "  int dims;\n" 
    "} TensorInfoCl;\n" 
    "// Contiguous tensors of more than one dimension are collapsed down\n" 
    "// to one tensor\n" 
    "bool TensorInfo_isContiguous( TensorInfoCl tensorInfo ) {\n" 
    "    return (tensorInfo.dims == 1 && tensorInfo.strides[0] == 1);\n" 
    "}\n" 
    "\n" 
    "void op1( global float *out ) {\n" 
    "    {{operation}};\n" 
    "}\n" 
    "\n" 
    "// Translate a linear index for the apply to a float* offset;\n" 
    "// specialized on `Dims` to reduce nvcc compilation time\n" 
    "{% for _,dim in ipairs(dims) do %}\n" 
    "int IndexToOffset_{{1000 + dim}}_get( int linearId, TensorInfoCl info) {\n" 
    "  int offset = 0;\n" 
    "\n" 
    "  // Use static dims\n" 
    "  for (int i = {{dim}} - 1; i >= 0; --i) {\n" 
    "    int curDimIndex = linearId % info.sizes[i];\n" 
    "    int curDimOffset = curDimIndex * info.strides[i];\n" 
    "    offset += curDimOffset;\n" 
    "\n" 
    "    if (i > 0) {\n" 
    "      linearId /= info.sizes[i];\n" 
    "    }\n" 
    "  }\n" 
    "\n" 
    "  return offset;\n" 
    "}\n" 
    "{% end %}\n" 
    "\n" 
    "int IndexToOffset_998_get(int linearId, const TensorInfoCl info) {\n" 
    "    return linearId;\n" 
    "}\n" 
    "\n" 
    "int IndexToOffset_999_get(int linearId, const TensorInfoCl info) {\n" 
    "  int offset = 0;\n" 
    "\n" 
    "  // Use dynamic dims\n" 
    "  for (int i = info.dims - 1; i >= 0; --i) {\n" 
    "    int curDimIndex = linearId % info.sizes[i];\n" 
    "    int curDimOffset = curDimIndex * info.strides[i];\n" 
    "    offset += curDimOffset;\n" 
    "\n" 
    "    linearId /= info.sizes[i];\n" 
    "  }\n" 
    "\n" 
    "  return offset;\n" 
    "}\n" 
    "\n" 
    "kernel void\n" 
    "THClTensor_pointwiseApply1(global TensorInfoCl *a,\n" 
    "                            global float* a_data,\n" 
    "                             int totalElements) {\n" 
    "  // these are mostly just to help me work out the conversions\n" 
    "  // than to actually use:\n" 
    "  // int blockDim_x = get_local_size(0);\n" 
    "  // int blockIdx_x = get_group_id(0);\n" 
    "  // int threadIdx_x = get_local_id(0);\n" 
    "  // int gridDim_x = get_num_groups(0);\n" 
    "  // blockIdx.x * blockDim.x + threadIdx.x\n" 
    "  //\n" 
    "  // = get_group_id(0) * get_local_size(0) + get_local_id(0)\n" 
    "  //  = get_global_id(0) ?\n" 
    "  //\n" 
    "  // gridDim.x * blockDim.x = get_num_groups(0) * get_local_size(0)\n" 
    "  //                        = get_global_size(0)\n" 
    "  //\n" 
    "  // for (int linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n" 
    "  for (int linearIndex = get_global_id(0); // I .... guess?\n" 
    "       linearIndex < totalElements;\n" 
    "       // linearIndex += gridDim.x * blockDim.x) {\n" 
    "       linearIndex += get_global_size(0) /* ? */ ) {\n" 
    "    // Convert `linearIndex` into an offset of `a`\n" 
    "    const int aOffset =\n" 
    "      IndexToOffset_{{1000+adim}}_get(linearIndex, a[0]);\n" 
    "\n" 
    "    op1( &(a_data[aOffset + a->offset]) );\n" 
    "  }\n" 
    "}\n" 
    "\n" 
    "";
    // [[[end]]]
    return kernelSource;
}

std::string getApply2_template() {
    // [[[cog
    // import stringify
    // stringify.write_kernel( "kernel", "THClApply2.cl" )
    // ]]]
    // generated using cog, from THClApply2.cl:
    const char * kernelSource =  
    "// OpenCL kernels....\n" 
    "\n" 
    "// expected templated values:\n" 
    "// dims (vector of unique dimension values)\n" 
    "// operation\n" 
    "// adim\n" 
    "// bdim\n" 
    "// cdim\n" 
    "//\n" 
    "// maybe should add:\n" 
    "// IndexType (hardcoded to int for now)\n" 
    "// MAX_CUTORCH_DIMS (hardcoded to 25 for now)\n" 
    "\n" 
    "// (Ported from cutorch's THCApply.cuh)\n" 
    "\n" 
    "// Maximum number of dimensions allowed for cutorch\n" 
    "// #define MAX_CUTORCH_DIMS 25\n" 
    "\n" 
    "// Enum that indicates whether tensor arguments are read/write or\n" 
    "// read-only\n" 
    "//enum TensorArgType { ReadWrite, ReadOnly };\n" 
    "\n" 
    "// kernel argument that defines tensor layout\n" 
    "typedef struct TensorInfoCl {\n" 
    "  // Extracts size/stride information for the kernel.\n" 
    "  // Successive dimensions can be collapsed if the size/strides match\n" 
    "  // up and thus there are no holes between the dimensions. This is used\n" 
    "  // to reduce the complexity of the problem.\n" 
    "  // The optional `reduceDim` indicates a reduction dimension for the\n" 
    "  // given tensor, so that the output size for this dimension will be 1.\n" 
    "\n" 
    "  int sizes[{{MAX_CLNN_DIMS}}];\n" 
    "  int strides[{{MAX_CLNN_DIMS}}];\n" 
    "  int offset;\n" 
    "  int dims;\n" 
    "} TensorInfoCl;\n" 
    "// Contiguous tensors of more than one dimension are collapsed down\n" 
    "// to one tensor\n" 
    "bool TensorInfo_isContiguous( TensorInfoCl tensorInfo ) {\n" 
    "    return (tensorInfo.dims == 1 && tensorInfo.strides[0] == 1);\n" 
    "}\n" 
    "\n" 
    "void op2( global float *out, global float *in1 ) {\n" 
    "    {{operation}};\n" 
    "}\n" 
    "\n" 
    "// Translate a linear index for the apply to a float* offset;\n" 
    "// specialized on `Dims` to reduce nvcc compilation time\n" 
    "{% for _,dim in ipairs(dims) do %}\n" 
    "int IndexToOffset_{{1000 + dim}}_get( int linearId, TensorInfoCl info) {\n" 
    "  int offset = 0;\n" 
    "\n" 
    "  // Use static dims\n" 
    "  for (int i = {{dim}} - 1; i >= 0; --i) {\n" 
    "    int curDimIndex = linearId % info.sizes[i];\n" 
    "    int curDimOffset = curDimIndex * info.strides[i];\n" 
    "    offset += curDimOffset;\n" 
    "\n" 
    "    if (i > 0) {\n" 
    "      linearId /= info.sizes[i];\n" 
    "    }\n" 
    "  }\n" 
    "\n" 
    "  return offset;\n" 
    "}\n" 
    "{% end %}\n" 
    "\n" 
    "int IndexToOffset_998_get(int linearId, const TensorInfoCl info) {\n" 
    "    return linearId;\n" 
    "}\n" 
    "\n" 
    "int IndexToOffset_999_get(int linearId, const TensorInfoCl info) {\n" 
    "  int offset = 0;\n" 
    "\n" 
    "  // Use dynamic dims\n" 
    "  for (int i = info.dims - 1; i >= 0; --i) {\n" 
    "    int curDimIndex = linearId % info.sizes[i];\n" 
    "    int curDimOffset = curDimIndex * info.strides[i];\n" 
    "    offset += curDimOffset;\n" 
    "\n" 
    "    linearId /= info.sizes[i];\n" 
    "  }\n" 
    "\n" 
    "  return offset;\n" 
    "}\n" 
    "\n" 
    "kernel void\n" 
    "THClTensor_pointwiseApply2(global TensorInfoCl *a,\n" 
    "                             global TensorInfoCl *b,\n" 
    "                            global float* a_data,\n" 
    "                            global float*b_data,\n" 
    "                             int totalElements) {\n" 
    "  // these are mostly just to help me work out the conversions\n" 
    "  // than to actually use:\n" 
    "  // int blockDim_x = get_local_size(0);\n" 
    "  // int blockIdx_x = get_group_id(0);\n" 
    "  // int threadIdx_x = get_local_id(0);\n" 
    "  // int gridDim_x = get_num_groups(0);\n" 
    "  // blockIdx.x * blockDim.x + threadIdx.x\n" 
    "  //\n" 
    "  // = get_group_id(0) * get_local_size(0) + get_local_id(0)\n" 
    "  //  = get_global_id(0) ?\n" 
    "  //\n" 
    "  // gridDim.x * blockDim.x = get_num_groups(0) * get_local_size(0)\n" 
    "  //                        = get_global_size(0)\n" 
    "  //\n" 
    "  // for (int linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n" 
    "  for (int linearIndex = get_global_id(0); // I .... guess?\n" 
    "       linearIndex < totalElements;\n" 
    "       // linearIndex += gridDim.x * blockDim.x) {\n" 
    "       linearIndex += get_global_size(0) /* ? */ ) {\n" 
    "    // Convert `linearIndex` into an offset of `a`\n" 
    "    const int aOffset =\n" 
    "      IndexToOffset_{{1000+adim}}_get(linearIndex, a[0]);\n" 
    "\n" 
    "    // Convert `linearIndex` into an offset of `b`\n" 
    "    const int bOffset =\n" 
    "      IndexToOffset_{{1000+bdim}}_get(linearIndex, b[0]);\n" 
    "\n" 
    "    op2( &(a_data[aOffset + a->offset]), &(b_data[bOffset + b->offset]));\n" 
    "  }\n" 
    "}\n" 
    "\n" 
    "";
    // [[[end]]]
    return kernelSource;
}

std::string getApply3_template() {
    // [[[cog
    // import stringify
    // stringify.write_kernel( "kernel", "THClApply3.cl" )
    // ]]]
    // generated using cog, from THClApply3.cl:
    const char * kernelSource =  
    "// OpenCL kernels....\n" 
    "\n" 
    "// expected templated values:\n" 
    "// dims (vector of unique dimension values)\n" 
    "// operation\n" 
    "// adim\n" 
    "// bdim\n" 
    "// cdim\n" 
    "//\n" 
    "// maybe should add:\n" 
    "// IndexType (hardcoded to int for now)\n" 
    "// MAX_CUTORCH_DIMS (hardcoded to 25 for now)\n" 
    "\n" 
    "// (Ported from cutorch's THCApply.cuh)\n" 
    "\n" 
    "// Maximum number of dimensions allowed for cutorch\n" 
    "// #define MAX_CUTORCH_DIMS 25\n" 
    "\n" 
    "// Enum that indicates whether tensor arguments are read/write or\n" 
    "// read-only\n" 
    "//enum TensorArgType { ReadWrite, ReadOnly };\n" 
    "\n" 
    "// kernel argument that defines tensor layout\n" 
    "typedef struct TensorInfoCl {\n" 
    "  // Extracts size/stride information for the kernel.\n" 
    "  // Successive dimensions can be collapsed if the size/strides match\n" 
    "  // up and thus there are no holes between the dimensions. This is used\n" 
    "  // to reduce the complexity of the problem.\n" 
    "  // The optional `reduceDim` indicates a reduction dimension for the\n" 
    "  // given tensor, so that the output size for this dimension will be 1.\n" 
    "\n" 
    "  int sizes[{{MAX_CLNN_DIMS}}];\n" 
    "  int strides[{{MAX_CLNN_DIMS}}];\n" 
    "  int offset;\n" 
    "  int dims;\n" 
    "} TensorInfoCl;\n" 
    "// Contiguous tensors of more than one dimension are collapsed down\n" 
    "// to one tensor\n" 
    "bool TensorInfo_isContiguous( TensorInfoCl tensorInfo ) {\n" 
    "    return (tensorInfo.dims == 1 && tensorInfo.strides[0] == 1);\n" 
    "}\n" 
    "\n" 
    "void op3( global float *out, global float *in1, global float *in2 ) {\n" 
    "    {{operation}};\n" 
    "}\n" 
    "\n" 
    "// Translate a linear index for the apply to a float* offset;\n" 
    "// specialized on `Dims` to reduce nvcc compilation time\n" 
    "{% for _,dim in ipairs(dims) do %}\n" 
    "int IndexToOffset_{{1000 + dim}}_get( int linearId, TensorInfoCl info) {\n" 
    "  int offset = 0;\n" 
    "\n" 
    "  // Use static dims\n" 
    "  for (int i = {{dim}} - 1; i >= 0; --i) {\n" 
    "    int curDimIndex = linearId % info.sizes[i];\n" 
    "    int curDimOffset = curDimIndex * info.strides[i];\n" 
    "    offset += curDimOffset;\n" 
    "\n" 
    "    if (i > 0) {\n" 
    "      linearId /= info.sizes[i];\n" 
    "    }\n" 
    "  }\n" 
    "\n" 
    "  return offset;\n" 
    "}\n" 
    "{% end %}\n" 
    "\n" 
    "int IndexToOffset_998_get(int linearId, const TensorInfoCl info) {\n" 
    "    return linearId;\n" 
    "}\n" 
    "\n" 
    "int IndexToOffset_999_get(int linearId, const TensorInfoCl info) {\n" 
    "  int offset = 0;\n" 
    "\n" 
    "  // Use dynamic dims\n" 
    "  for (int i = info.dims - 1; i >= 0; --i) {\n" 
    "    int curDimIndex = linearId % info.sizes[i];\n" 
    "    int curDimOffset = curDimIndex * info.strides[i];\n" 
    "    offset += curDimOffset;\n" 
    "\n" 
    "    linearId /= info.sizes[i];\n" 
    "  }\n" 
    "\n" 
    "  return offset;\n" 
    "}\n" 
    "\n" 
    "kernel void\n" 
    "THClTensor_pointwiseApply3(global TensorInfoCl *a,\n" 
    "                             global TensorInfoCl *b,\n" 
    "                            global TensorInfoCl *c,\n" 
    "                            global float* a_data,\n" 
    "                            global float*b_data,\n" 
    "                            global float*c_data,\n" 
    "                             int totalElements) {\n" 
    "  // these are mostly just to help me work out the conversions\n" 
    "  // than to actually use:\n" 
    "  // int blockDim_x = get_local_size(0);\n" 
    "  // int blockIdx_x = get_group_id(0);\n" 
    "  // int threadIdx_x = get_local_id(0);\n" 
    "  // int gridDim_x = get_num_groups(0);\n" 
    "  // blockIdx.x * blockDim.x + threadIdx.x\n" 
    "  //\n" 
    "  // = get_group_id(0) * get_local_size(0) + get_local_id(0)\n" 
    "  //  = get_global_id(0) ?\n" 
    "  //\n" 
    "  // gridDim.x * blockDim.x = get_num_groups(0) * get_local_size(0)\n" 
    "  //                        = get_global_size(0)\n" 
    "  //\n" 
    "  // for (int linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n" 
    "  for (int linearIndex = get_global_id(0); // I .... guess?\n" 
    "       linearIndex < totalElements;\n" 
    "       // linearIndex += gridDim.x * blockDim.x) {\n" 
    "       linearIndex += get_global_size(0) /* ? */ ) {\n" 
    "    // Convert `linearIndex` into an offset of `a`\n" 
    "    const int aOffset =\n" 
    "      IndexToOffset_{{1000+adim}}_get(linearIndex, a[0]);\n" 
    "\n" 
    "    // Convert `linearIndex` into an offset of `b`\n" 
    "    const int bOffset =\n" 
    "      IndexToOffset_{{1000+bdim}}_get(linearIndex, b[0]);\n" 
    "\n" 
    "    const int cOffset =\n" 
    "      IndexToOffset_{{1000+cdim}}_get(linearIndex, c[0]);\n" 
    "\n" 
    "    op3(\n" 
    "      &(a_data[aOffset + a->offset]),\n" 
    "      &(b_data[bOffset + b->offset]),\n" 
    "      &(c_data[cOffset + c->offset])\n" 
    "    );\n" 
    "  }\n" 
    "}\n" 
    "\n" 
    "";
    // [[[end]]]
    return kernelSource;
}

